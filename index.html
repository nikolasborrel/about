<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">

<head>
    <meta charset="utf-8">    
    <meta name="description" content="Overview of my research at the Acoustic Technologies group at DTU Elektro within Virtual Acoustics (SEM, DG-FEM, WBM, domain decomposition methods). Machine learning in the field of music and audio is also of great interest to me."/>
    <title>Nikolas Borrel-Jensen</title>
</head>

<body>

<hr>
<CENTER>
    [ <a href="#summary">Summary</a> 
    | <a href="#research_interests">Research Interests</a>
    | <a href="#workexperience">Work experience</a>
    | <a href="#education">Education</a>
    | <a href="#theses">Theses</a>
    | <a href="#projects">Projects</a>
    | <a href="#talks">Talks and awards</a>
    | <a href="#teaching">Teaching</a>
    | <a href="#courses">PhD courses</a>
    | <a href="#contact">Contact</a> ]
</CENTER>
<hr>

<h2>Summary</h2>
<a name="summary"></a>

Nikolas Borrel-Jensen, Master in Computer Science, University of Copenhagen (2012)

<p>

PhD student at <a href="https://www.dtu.dk/service/telefonbog/person?id=50173" target="_blank">DTU Elektro, Acoustic Technologies</a> (2019-2022). <br>
Supervisors: Jeong, Cheol-Ho (DTU); Engsig-Karup, Allan Peter (DTU); Hornikx, Maarten (TU Eindhoven)

<hr>

<h2>Research Interests</h2>
<a name="research_interests"></a>

<h3>Virtual acoustics</h3>
I am concerned with developing accurate and efficient methods for simulating room acoustics. Applications are in building design, computer games, and mixed reality. The focus is centered around numerical methods solving the underlying physics, implicitly taking wave phenomena into account. Still, it comes with the price of being computationally expensive, especially for larger rooms and for higher frequencies. Therefore, an offline pre-processing step is done to calculate the room impulse responses in a grid, which are then convolved with the source signal at runtime.

<p>
Currently, I am researching topics within the following areas

<ul>
    <li> <a href="https://en.wikipedia.org/wiki/Discontinuous_Galerkin_method" target="_blank">SEM/DG-FEM</a>
    <li> <a href="https://www.mech.kuleuven.be/en/doctorates/desmet.pdf" target="_blank">WBM (Wave-based method)</a>
    <li> <a href="https://en.wikipedia.org/wiki/Domain_decomposition_methods" target="_blank">Domain decomposition methods</a>
</ul>

<h3>Algorithmic composition</h3>
In addition, I have a profound interest in algorithmic composition, such as data-driven music creation and harmonization. Previously, I developed a method for harmonising rhythmic music. It uses a hidden Markov model to learn harmonisations of different artists in different genres and allows new chord sequences to be generated with respect to a given melody.
  
<ul>
    <li> Manuscript: <a href="src/cremo/cremo.pdf" target="_blank">Harmonisation in modern rhythmic music using Hidden Markov Models</a>
    <li> Talk: <a href="https://youtu.be/tc22n7j7ixY" target="_blank">Harmonisation in modern rhythmic music using Hidden Markov Models (Audio Developer Conference, London, 2017)</a>
    <li> Code: <a href="https://github.com/cremo-music/Notus" target="_blank">Domain-specific language for expressing musical structures in the high-level, declarative style of functional programming</a>
</ul>

Audio examples
<p>
<audio controls> <source src="src/cremo/burma.m4a" type="audio/mpeg"> </audio> 
<audio controls> <source src="src/cremo/prinsnana.m4a" type="audio/mpeg"> </audio>
<audio controls> <source src="src/cremo/munk.m4a" type="audio/mpeg"> </audio>

<h2>Work experience</h2>
<a name="workexperience"></a>
<ul>
    <li>2019-: <b>PhD student, Virtual Acoustics, DTU - Technical University of Denmark</b>.
        Developing and implementing numerical methods for solving the physical equations describing the wave behavior and boundary conditions.
    <li>2019-2019: <b>Blockchain Software Developer, eToroX (DK)</b>.
        Developing blockchain solutions for financial trading, collaborating with academia to solve advanced problems in the field.
    <li>2018-2019: <b>Product Owner/Software Developer, Mobile Solutions, 3Shape (DK)</b>.
        Leading the Mobile Solutions at 3Shape, one of the world's leading manufacturer of 3D scanners and CAD/CAM software solutions within the dental industry.
    <li>2013-2018: <b>Mobile Software Developer, 3Shape (DK)</b>.
        I took over a very young project, re-designed the app and grew the active user base from almost none to a substantial number of users. Leading transition from ObjC/Swift to Xamarin C# involving architectural decisions.
    <li>2012-2015: <b>3D Scanner Software Developer, 3Shape (DK)</b>.
        Part of a team of ten people developing 3D dental scanning software written in Delphi/C#.
    <li>2015-2018: <b>Founder, Livetake (DK)</b>.
        Founded the award-winning company <a href="https://livetake.wixsite.com/livetake20" target="_blank">Livetake</a>, making it possible to co-create stories with friends and automatically replace the poor smartphone sound with high quality live sound. Developed the technology stack on my own.
        <ul>
            <li>Launched at Roskilde Festival 2015 (100.000+ visitors), won a TechCrunch Disrupt award in San Francisco (2015), won the Danish Sound Startup Award (2016).
            <li> Partnerships, strategy, business plans, hiring, user tests.
            <li> Web service in Golang; iOS app for synchronizing videos with high-quality audio; <a href="https://livetake.wixsite.com/livemeister" target="_blank">iOS audio app (DAW)</a> for recording multi-channel concert audio
        </ul>
</ul>

<h2>Education</h2>
<a name="education"></a>
<ul>
    <li>2010-2012: <b>MSc in Computer Science, Uni. of Copenhagen, Denmark</b>. Specialized in Mathematical Modelling and Computations.
    <li>2006-2010: <b>BSc in Computer Science, Uni. of Copenhagen, Denmark</b>.
    <li>2005-2006: <b>Carl Nielsen Academy of Music, Music Concervatory, Odense, Denmark</b>. Completed first year at the rhythmic section with the guitar as my main instrument.
</ul>

<h2>Theses</h2>
<a name="theses"></a>
<ul>
    <li><a href="src/borrel_jensen_masters_thesis_revised.pdf" target="_blank"> Master's Thesis</a> (2012): <b>Real-time Auralisation of the Lower Frequency Sound Field using Numerical Methods on the GPU</b>, Institute for Technical Acoustics, Aachen Uni., DE / Dep. of Computer Science, Uni. of Copenhagen, DK. 
        <p>The thesis investigated whether it was possible to implement finite-difference time-domain methods for solving the 3-D wave equation in real-time on the GPU using C++/CUDA with focus on physical correct simulations of the lower frequency sound field.
        
    <li>Bachelor's Thesis (2009): <b>Computer-assisted music composition -- A database-backed algorithmic composition system</b>, Dep. of Computer Science, Uni. of Copenhagen, DK. 
        <p>Bachelor thesis on the topic 'algorithmic composition' using different machine learning techniques. A software tool was developed for automatically generating music phrase variation and harmonisation from user input, each conforming to a predefined musical genre.
    
</ul>

<h2>Projects</h2>
<a name="projects"></a>
<ul>
    <li><b>San Diego (music)</b> <a href="https://soundcloud.com/nikolas-borrel-jensen/sets/san-diego-sketches" target="_blank">[SoundCloud]</a>
        <p>I have started composing and playing some music again after some years of hibernation. Some rough sketches for my project San Diego can be found on SoundCloud.
    <li><b>Notus for Swift, Cremo Music (2019)</b> <a href="https://github.com/cremo-music/Notus" target="_blank">[github]</a>
        <p>Notus is a domain-specific language for expressing musical structures in the high-level, declarative style of functional programming written in Swift.
    <li> <b>Harmonisation in modern rhythmic music using Hidden Markov Models</b><a href="src/cremo/cremo.pdf" target="_blank"> [manuscript]</a>
        <p> A method for harmonising rhythmic music is presented. It uses a hidden Markov model to learn harmonisations of different artists in different genres and allows new chord sequences to be generated with respect to a given melody.
</ul>
<h2>Talks and awards</h2>
<a name="talks"></a>
<ul>
    <li> <b>Invited speaker at Primavera Pro</b>, Barcelona, Spain (2018). Gave a 25 minutes talk about Livetake and how to engage concertgoers.
    <li> <b>Keynote speaker at Audio Developer Conference</b>, London (2017) <a href="https://youtu.be/tc22n7j7ixY" target="_blank">[video]</a>
        <p>
        A method for harmonising rhythmic music, implemented in Haskell, will be presented. It uses a hidden Markov model to learn harmonisations of different artists in different genres and allows new chord sequences to be generated with respect to a given melody. The main focus of this talk is on how to perform the feature extraction for the chords and the melody lines necessary for the method to perform well.        
    <li> <b>Eventbrite Live Award at TechCrunch Disrupt San Francisco (2015)</b><a href="https://techcrunch.com/video/eventbrite-award" target="_blank"> [video]</a>
        <p>
        Livetake participated in the TechCrunch Disrupt Event 2015 in San Francisco and demonstrated their latest technology. The prize for the technology that most enhance live concert experiences was given to Livetake.
    <li> <b>Danish Sound Startup Award (2016)</b>
        <p>
        Livetake won the prize for the most promising startup in Denmark within the audio industry. 
        The award was given by the <a href="https://danishsound.org" target="_blank">Danish Sound Network</a>, an innovation network with the aim of connecting start-ups, established companies and knowledge institutions within the Danish Sound industry.
</ul>

<h2>Teaching</h2>
<a name="teaching"></a>
Download the proposed project descriptions <a href="src/projects_VA.pdf" target="_blank"> here</a>. <br> <br>

The project proposals are all centered around the topic `Real-time sound simulations for interactive scenes' with applications in computer games and mixed reality. In interactive scenes, sources and receivers are allowed to move freely, and scene changes should be possible, for example, opening and closing doors, and moving obstacles around. The project proposals are divided into four parts: 

<ul>
    <li> A) modeling interactive scenes (requires good math/programming skills)
    <li> B) material modeling (requires good math/programming skills, knowledge in acoustics)
    <li> C) efficient implementations (requires excellent programming skills)
    <li> D) method evaluation (requires good knowledge in acoustics)
</ul>

<h4>Project 1a: Modeling flexible geometries at runtime</h4>
<p>
    The impulse responses are calculated offline for fixed positions, restricting the flexibility of the scene at runtime. However, in most real-time environments, some modification to the scene should be possible, for example, opening and closing doors, and moving obstacles around. This project should investigate efficient methods for allowing (some) geometrical flexibility at runtime without pre-computing all possible settings, but instead learning to adapt to changes from few pre-processed settings. The methods should be computation efficient and require low storage, yet still exhibiting physically correct results.
</p>

<h4>Project 2a: Impulse response interpolations</h4>
<p>
    The impulse responses are calculated offline for fixed positions in the scene. At runtime, the receivers are allowed to move freely, also at locations where no impulse responses have been calculated. Simple linear interpolations have shown degraded accuracy and therefore more advanced methods should be investigated. A coarser grid means less storage (which can be hundreds of gigabytes in complex scenes) but requires better reconstruction methods. The goal of this project is to develop methods for reconstructing the sound field between the fixed impulse responses.
</p>

<h4>Project 3a: Impulse response compression</h4>
<p>
    The impulse responses for the scenes are pre-calculated for fixed grid position and the storage requirements can be huge in large, complex scenes (up to hundreds of gigabytes). The goal of this project is to compress the impulse responses, taking human perception into account by considering early and late reflections.
</p>

<h4>Project 4a: Aesthetic modification of room acoustics for interactive auralization</h4>
<p>
    On the contrary to architectural acoustics where the goal is to accurately simulate the real-world, games and mixed reality require aesthetic adjustments to the simulated acoustic properties, still requiring realistic acoustics. Room conditions should be adjusted e.g. to increase the intelligibility of important scripted dialogs, or to enhance the perception of a large room by increasing the reverberation time. This project aims to modify the accurately simulated impulse responses to match some required aesthetics in a simple, efficient, and automized manner.
</p>

<h4>Project 1b: Modeling sound transmission</h4>
<p>
    It is important to be able to model sound transmission through materials - such as walls - for realistic, immersive scenes. Sound transmissions can be divided into two types: airborne and structure-borne/impact sound transmission. Airborne transmission is concerned with sound sources in one room inducing vibrations from air pressure waves on one side of a separating wall impacting the structure to transmit the waves through the material. The other face of the structure starts vibrating, transferring the sound into the new domain. Structure-borne/impact transmission is concerned with a sound source in one room resulting from an object impacting a separating surface transmitting the sound to an adjacent room. An example could be footsteps on the floor being transmitted to a room below. This project is concerned with coming up with efficient methods for modeling sound transmission methods for efficient real-time, interactive scenes.
</p>

<h4>Project 2b: Open-space sound propagation</h4>
<p>
    Our methods for sound propagation have been implemented for room acoustics only, but a typical scenario in interactive scenes includes open-spaces - such as outdoor and semi-outdoor scenes. Possible factors to include could be air attenuations and wind impacting the sound field. The goal of this project is to efficiently and realistically simulate outdoor spaces including factors such as air attenuations and wind.
</p>

<h4>Project 1c: Implementation of the Wave-Based Method on Massively Parallel Systems</h4>
<p>
    The Wave-Based Method has been implemented by the group for simulating wave propagation in rooms and is a numerical method for solving the steady-state Helmholtz equation w.r.t. sound pressure. The method is computationally expensive especially for large rooms and high frequencies and therefore an efficient and high-performant implementation is needed for off-line calculations. The goal is to implement the existing method on massively parallel hardware, scalable to many cluster nodes and GPUs in a low-level language as C/C++/Rust and CUDA/AMD ROCm for GPU capabilities. Middle-layer languages for GPU programming, such as Futhark or OCCA can be considered.
</p>

<h4>Project 1d: Evaluate Project Triton: a tool for immersive sound propagation for games and mixed reality</h4>
<p>
    Microsoft has created “Project Triton” for immersive sound propagation for games and mixed reality and a plugin for Unity is available. This project should investigate how well Triton performs concerning some parameters, e.g.
    <ul>
        <li> Obstruction/occlusion (diffraction), reverberance/decay time
        <li> Source/receiver directivity
        <li> Compare Triton with geometrical acoustics (GA) in interactive scenes: when does GA break down? Investigate smooth transitions moving around occlusion/obstacles (GA has problems here due to the abrupt change of the direct path)
        <li> Ease of use: UI/UX, compare with the usual way of designing sound in games (using e.g. Wwise from Audiokinetic)
        <li> Computational efforts
    </ul>              
    3) is particularly interesting: The additional accuracy gained from wave physics might not be needed in computer games, so why use wave physics instead of GA? One reason is the smooth transitions when moving around the scene, where it is claimed that GA cannot ensure this.
</p>

<h2>Courses taken as part of my PhD study</h2>
<a name="courses"></a>
<ul>
    <li> <a href="https://kurser.dtu.dk/course/02623" target="_blank">The Finite Element Method for Partial Differential Equations (Januar 2019)</a>
    <li> <a href="https://kurser.dtu.dk/course/02689" target="_blank">Advanced Numerical Methods for Differential Equations (fall 2020)</a>
    <li> <a href="https://kurser.dtu.dk/course/02936" target="_blank">Bayesian Data Analysis (spring 2020)</a>    
    <li> <a href="https://kurser.dtu.dk/course/02456" target="_blank">Deep Learning (fall 2020)</a>    
    <li> <a href="https://kurser.dtu.dk/course/31280" target="_blank">Research Topics in Acoustic Technology (fall/spring 2020/2021)</a>    
    <li> <a href="https://kurser.dtu.dk/course/42750" target="_blank">Sustainability evaluation and communication (August 2020)</a>
    <li> How to write a scientific paper (Fall 2020)
</ul>

<h2>Contact</h2>
<a name="contact"></a>

<b>Nikolas Borrel-Jensen</b><br>
Department of Electrical Engineering, Technical University of Denmark<br>
Oersteds Plads, Building 352, room 004, 2800 Kgs. Lyngby<br>

<P>

E-mail: <a href="mailto: nibor@elektro.dtu.dk">nibor@elektro.dtu.dk</a> <br>
Phone: (+45) 60 95 85 24 <br>